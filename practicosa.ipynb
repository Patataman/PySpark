{
 "cells": [
  {
   "source": [
    "# **Práctica 2. Procesamiento de datos mediante Apache Spark**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Autores**:\n",
    "\n",
    "- Carlos Vigil González                 100363974\n",
    "- David Gil López                       100363815\n",
    "- Daniel Alejandro Rodríguez López      100316890\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introducción \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "En este trabajo se van a analizar una serie de datos sobre los Taxis de la Ciudad de Nueva York utilizando Apache Spark y Python (PySpark), todo ello ejecutado en este Jupyter Notebook.\n",
    "\n",
    "***\n",
    "En este caso se ha utilizado la versión 3 de Apache Spark para el desarrollo del código y sus pruebas.\n",
    "***\n",
    "\n",
    "En los apartados siguientes se van a ir comentando diferentes aspectos de la práctica, empezando con una limpieza de los datos de entrada, continuando con los diferentes análisis de dichos datos, para acabar con unas conclusiones sobre los tiempos de ejecución de cada estudio y sus velocidades, comparándolas entre si para ver por qué unos se ejecutan más rápido que otros."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Código para ejecutar en google collab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "Aquí dejamos el código para ejecutar el notebook en google collab. Para ello, descomentar el bloque de abajo y quitar el \"import os\" repetido en el bloque de Imports.\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "#!tar xf /content/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "#!pip install -q findspark\n",
    "#\n",
    "#import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "is_conda = os.path.exists(os.path.join(sys.prefix, 'conda-meta'))\n",
    "\n",
    "if not is_conda:\n",
    "    import findspark \n",
    "    findspark.init()\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, unix_timestamp\n",
    "from time import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Para una lectura más distendida de la memoria\n",
    "MODO_JAJAS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "source": [
    "En este apartado se incluye una función para facilitar la creación de las posteriores gráficas y la creación del entorno de spark con la lectura del CSV inicial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects):\n",
    "    # https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", fontsize=25,\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://giphy.com/embed/I1U9DTjCqOF3i\",width=\"240\", height=\"135\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"taxis\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df = spark.read.csv('./tripdata_2017_01.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "dfP=df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "source": [
    "En este apartado se limpian los datos de entrada, para qeu a la hora de procesarlos no salgan valores fuera de lo esperado."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://giphy.com/embed/xsATxBQfeKHCg\", width=\"240\", height=\"180\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data_count = df.count()\n",
    "print(\"Cantidad de datos usados:\", initial_data_count)\n",
    "display(dfP)\n",
    "display(dfP.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos extraños en el dataset\n",
    "\n",
    "Lista de comportamientos extraños en los datos, y por tanto, inválidos a la hora de utilizarlos, ya que deberían ser coherentes basándonos en la información de cada campo proporcionada por la [documentación](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)\n",
    "\n",
    "* Existen carreras en las que la distancia es 0\n",
    "* Existen propinas negativas\n",
    "* \"extra\" con valores diferentes a 0 (ya que puede no haber extras), 0.5 y 1\n",
    "* Existen viajes con un precio final negativo\n",
    "* \"MTA_tax\" debe valer siempre 0.50. Valores diferentes son erróneos, y por tanto puede que el resto de la información también\n",
    "* \"Improvement_surcharge\" es un valor en desuso, por lo que debería valer en el menor caso 0, no -0.3\n",
    "* Carreras cuya fecha de fin sea igual o anterior a la fecha de inicio\n",
    "* Existen tarifas con valores negativos. No tiene sentido ya que la tarifa va en función del tiempo y la distancia recorridas\n",
    "\n",
    "### Elementos extraños PERO posibles\n",
    "\n",
    "* Número de pasajeros es 0. Dado que es un valor que introduce el propio conductor, valores como 0 son posibles si al conductor le da igual introducir bien el valor.\n",
    "* Un viaje empieza y acaba en la misma zona. Puede deberse a zonas grandes o que son viajes de ida y vuelta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza realizada\n",
    "\n",
    "A partir de los comportamientos observados se ha procedido a eliminar las carreras que cumplen las siguientes condiciones:\n",
    "\n",
    "- Campo \"tip_amount\" con valores menor a 0\n",
    "- Campo \"total_amount\" con valores menor o igual a 0\n",
    "- Campo \"trip_distance\" con valores menor o igual a 0\n",
    "- Campo \"fare_amount\" con valores menor o igual a 0\n",
    "- Campo \"extra\" con valores diferentes de 0, 0.5 y 1\n",
    "- Campo \"MTA_tax\" con valor distinto de 0.5\n",
    "- Campo \"Improvement_surcharge\" con valor distinto de 0 o 0.3\n",
    "- Campo \"tpep_dropoff_datetime\" es anterior o igual a \"tpep_pickup_datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las fechas a timestamp, para que dejen de ser strings a secas\n",
    "# y guardamos su diferencia para luego tener más fácil el filtrado y otros cálculos\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"tpep_pickup_timestamp\", unix_timestamp(col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"tpep_dropoff_timestamp\", unix_timestamp(col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"time_diff\", col(\"tpep_dropoff_timestamp\") - col(\"tpep_pickup_timestamp\")  # Segundos\n",
    ")\n",
    "\n",
    "df.createOrReplaceTempView('datosCarreras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datosLimpios = spark.sql(\"\"\"\n",
    "    SELECT * FROM datosCarreras WHERE\n",
    "        tip_amount >= 0 AND\n",
    "        total_amount > 0 AND\n",
    "        trip_distance > 0 AND\n",
    "        fare_amount > 0 AND\n",
    "        (extra == 0 OR extra == 0.5 OR extra == 1) AND\n",
    "        mta_tax == 0.5 AND\n",
    "        improvement_surcharge >= 0 AND\n",
    "        time_diff > 0\n",
    "\"\"\")\n",
    "datosLimpios_count = datosLimpios.count()\n",
    "datosLimpios.createOrReplaceTempView('datosCarrerasLimpios')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(datosLimpiosP)\n",
    "# display(datosLimpiosP.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de información\n",
    "\n",
    "Ahora que ya hemos limpiado los datos y tenemos entradas coherentes, se puede proceder a extraer información de los mismos. \n",
    "\n",
    "La información que se va a extraer es:\n",
    "\n",
    "* Velocidad media de los taxis en función de la hora.\n",
    "* Viajes en taxi más comunes\n",
    "* Registros financieros (propinas, personas, etc.)\n",
    "    * Timos a turistas\n",
    "    * Propinas en función de la hora\n",
    "    * Identificar pasajeros borrachos\n",
    "* Zonas con poca cobertura\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bajamos el csv con la información de las zonas para luego poder \"traducir\"\n",
    "if not os.path.exists(\"taxi+_zone_lookup.csv\"):\n",
    "    !wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade al dataframe los datos de las zonas de subida y bajada. Para ello se va a realizar un join,\n",
    "# De forma que vaya dentro del dataframe y sea accesible también desde un rdd\n",
    "\n",
    "df_lookup = spark.read.csv('./taxi+_zone_lookup.csv', header=True, inferSchema=False)\n",
    "\n",
    "datosLimpios = datosLimpios.withColumn(\n",
    "    \"LocationID\", col(\"PULocationID\")\n",
    ").join(\n",
    "    # Renombramos para poder luego incluir también las de bajada\n",
    "    df_lookup.withColumnRenamed(\"Borough\", \"PUBorough\"\n",
    "            ).withColumnRenamed(\"Zone\", \"PUZone\"\n",
    "            ).withColumnRenamed(\"service_zone\", \"PUservice_zone\"),\n",
    "    on=['LocationID']\n",
    ").withColumn(\n",
    "    \"LocationID\", col(\"DOLocationID\")\n",
    ").join(\n",
    "    df_lookup.withColumnRenamed(\"Borough\", \"DOBorough\"\n",
    "            ).withColumnRenamed(\"Zone\", \"DOZone\"\n",
    "            ).withColumnRenamed(\"service_zone\", \"DOservice_zone\"),\n",
    "    on=['LocationID']\n",
    ")\n",
    "\n",
    "datosLimpios.createOrReplaceTempView('datosCarrerasLimpios')\n",
    "\n",
    "# datosLimpiosP = datosLimpios.toPandas()\n",
    "# display(datosLimpiosP)\n",
    "# display(datosLimpiosP.describe().T)\n",
    "t_limpieza = time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocidad media de los taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se realizará un análisis de la velocidad media de los taxis, para ello se realizará una transformación de millas a metros sabiendo que 1 milla = 1609.344 metros luego dividiéndolo entre la diferencia de tiempo calculada previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://www.youtube.com/embed/4vlVN5r7sKg\", width=\"360\", height=\"202\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0velocidades = time()\n",
    "dfMTS = datosLimpios.withColumn(\n",
    "    \"mean_speed\", col(\"trip_distance\")*1609.344/col(\"time_diff\")\n",
    ")\n",
    "dfMTS.createOrReplaceTempView('datosCarrerasLimpiosConVelocidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfMTSP = dfMTS.toPandas()\n",
    "t1velocidades = time()\n",
    "display(dfMTSP.sort_values(by=[\"mean_speed\"],ascending=False).head(50))\n",
    "display(dfMTSP.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vista de que las velocidades medias estaban mal debido a que el time_diff es muy bajo, probablemente por un error de los tiempos almacenador por los taxistas, se volverá a realizar una consulta eliminando tiempos menores a 3 minutos y se comprobará las velocidades promedio otra vez.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2velocidades = time()\n",
    "datosLimpiosSinVelocidades = spark.sql(\"SELECT * FROM datosCarrerasLimpios where time_diff >= 180\")\n",
    "dfMTS = datosLimpiosSinVelocidades.withColumn(\n",
    "    \"mean_speed\", col(\"trip_distance\")*1609.344/col(\"time_diff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfMTSP = dfMTS.toPandas()\n",
    "t3velocidades = time()\n",
    "display(dfMTSP.sort_values(by=[\"mean_speed\"],ascending=False).head(10))\n",
    "display(dfMTSP.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Como se puede observar, la mayoría de velocidades entre las 50 más rápidas superan el límite de velocidad nacional para zonas de carretera (24.72222 m/s) siendo que solo los 5 últimos lo cumplen, o en otras palabras que los 45 primeros infringen la ley.\n",
    "\n",
    "Por otro lado se puede ver que los 8 primeros tienen velocidades mayores a 52 metros por segundo, lo que implica velocidades de 187.2 km/s esto puede ser debido a que haya algún tipo de fallo en el tiempo o que lleve velocidades demasiado altas.\n",
    "\n",
    "Por último mencionar que los 6 primeros tienen velocidades mayores a 65 m/s, cosa que ya debe ser debido a un fallo, accidental o adrede por parte del conductor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfica velocidad media por hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4velocidades = time()\n",
    "velocidades_por_hora = spark.sql(\"\"\"\n",
    "    SELECT mean_speed, HOUR(tpep_dropoff_datetime) as hour \n",
    "      FROM datosCarrerasLimpiosConVelocidad\n",
    "      WHERE mean_speed <50\n",
    "\"\"\")\n",
    "\n",
    "velocidades_hora = velocidades_por_hora.groupBy(\"hour\").mean(\"mean_speed\").collect()\n",
    "\n",
    "t5velocidades=time()\n",
    "\n",
    "velocidades_x = [round(h,2) for h, cnt in velocidades_hora]\n",
    "velocidades_y = [round(cnt*3.6,1) for h, cnt in velocidades_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(velocidades_x, velocidades_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Velocidades (kmh)\", fontsize=25)\n",
    "ax.set_title(\"Velocidades medias por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar una grafica de velocidades medias por hora se puede observar que cuando las velocidades mas altas son a las horas nocturnas, probablemente porque hay menos trafico y menos vigilancia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_velocidad_media=t1velocidades-t0velocidades+t3velocidades-t2velocidades+t5velocidades-t4velocidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zonas de poca cobertura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la variable `store_and_fwd_flag`, creemos que es posible deducir qué zonas de la ciudad de Nueva York dan un mayor problema a la hora de estar conectados con el servidor de la compañía de taxis, es decir, tienen poca cobertura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://giphy.com/embed/PmdOx0iRRtqkBFlEgI\", width=\"240\", height=\"240\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_cobertura = time()\n",
    "sinCobertura_rdd = spark.sql(\"\"\"\n",
    "    SELECT DOBorough, DOZone\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE store_and_fwd_flag == 'Y'\n",
    "\"\"\").rdd\n",
    "\n",
    "# sC_rdd.flatMap(lambda x: x['locationID']).map(lambda x: (x,1))\n",
    "zone_tuples = sinCobertura_rdd.map(\n",
    "    lambda x: (x['DOZone'],1)\n",
    ").reduceByKey(\n",
    "    lambda x,y: x+y\n",
    ").sortBy(\n",
    "    lambda x: x[1], False\n",
    ")\n",
    "borough_tuples = sinCobertura_rdd.map(\n",
    "    lambda x: (x['DOBorough'],1)\n",
    ").reduceByKey(\n",
    "    lambda x,y: x+y\n",
    ").sortBy(\n",
    "    lambda x: x[1], False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonas = zone_tuples.take(10)\n",
    "distritos = borough_tuples.take(10)\n",
    "\n",
    "distritos_x = [k for k, v in distritos]\n",
    "distritos_y = [v for k, v in distritos]\n",
    "zonas_x = [k for k, v in zonas]\n",
    "zonas_y = [v for k, v in zonas]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "rects1 = ax.bar(distritos_x, distritos_y)\n",
    "ax.set_xlabel(\"Distrito\", fontsize=25)\n",
    "ax.set_ylabel(\"Registros guardados\", fontsize=25)\n",
    "ax.set_title(\"Registros guardados por distritos\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "rects2 = ax.bar(zonas_x, zonas_y)\n",
    "ax.set_xlabel(\"Zona\", fontsize=25)\n",
    "ax.set_ylabel(\"Registros guardados\", fontsize=25)\n",
    "ax.set_title(\"Registros guardados por zonas\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viajes_manhattan = spark.sql(\"\"\"\n",
    "    SELECT VendorID\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE DOBorough == 'Manhattan'\n",
    "\"\"\").count()\n",
    "int(distritos_y[0])/int(viajes_manhattan)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar por el número de registros guardados en los taxis, Manhattan es el distrito que tiene más registros guardados con 2490, casi 5 veces más que su distrito posterior, Queens con 435. Por lo que se puede deducir que tiene zonas frecuentadas en las que no se tiene cobertura.\n",
    "\n",
    "Sin embargo, es un porcentaje pequeño sobre el total, ya que representan menos del 0.3% de todos los viajes existentes que finalizan en Manhattan, por lo que muy probablemente se deba a taxis concretos que tienen problemas de conexión.\n",
    "\n",
    "Observando que se trata de un porcentaje muy bajo, surge otra pregunta: ¿En qué momento se guardaron los registros? ¿Todos los registros guardados son en horas similares y por tanto puede deberse a una caída del servidor más que a un problema de cobertura?\n",
    "\n",
    "Para ello se van a analizar los registros de Manhattan ya que son los más numerosos. Para ello nos quedaremos con el día en el que se han guardado el mayor número de mensajes y se analizará en qué horas se han guardado los mensajes, ya que de ser una caída del servidor, deberían agruparse alrededor de una hora concreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_manhattan = spark.sql(\"\"\"\n",
    "    SELECT tpep_dropoff_datetime, DAY(tpep_dropoff_datetime) as day, HOUR(tpep_dropoff_datetime) as hour\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE store_and_fwd_flag == 'Y' and DOBorough == 'Manhattan'\n",
    "\"\"\")\n",
    "\n",
    "# Nos vamos a quedar con el día con mayor número de registros guardados\n",
    "day, count = stored_manhattan.groupBy(\"day\").count().sort(\"count\").tail(1)[0]\n",
    "\n",
    "# Ahora que tenemos el día, podemos ver las horas en las que se agruparon los mensajes.\n",
    "# Si todos (o casi todos) los mensajes se agruparon en la misma hora, eso significa \n",
    "# (muy probablemente) que hubo una caída del servidor\n",
    "mensajes_hora = stored_manhattan.filter(col(\"day\")==day).groupBy(\"hour\").count().collect()\n",
    "t_cobertura = time() - t0_cobertura\n",
    "\n",
    "mensajes_x = [h for h, cnt in mensajes_hora]\n",
    "mensajes_y = [cnt for h, cnt in mensajes_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(mensajes_x, mensajes_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Mensajes\", fontsize=25)\n",
    "ax.set_title(\"Mensajes guardados por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "    Se ha tardado {round(t_cobertura, 2)}s en realizar este estudio sobre {datosLimpios_count},\n",
    "    lo que da un procesamiento de {round(datosLimpios_count/t_cobertura, 2)} dato/seg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mensajes guardados se agrupan entre las 10 AM y las 21 PM, pero dado que se corresponde también con el periodo de mayor actividad de los taxis, ya que es cuando la gran mayoría de la población se desplaza por la ciudad, no parece que se deba a una caída del servidor.\n",
    "\n",
    "Por lo tanto nos reafirmamos en la primera suposición: **Muy probablemente se debe a taxis concretos que tienen problemas de conexión**, no a una caída generalizada. Además si hubiese sido ese el caso, el número de registros debería haber sido muchísimo mayor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viajes más comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se realizará un estudio de los viajes más comunes a realizarse, para esto se tomarán del dataframe los datos referentes a las zonas de los viajes y con esto se hará un modelo sencillo de map-reduce para ver la cantidad de cada viaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0viajesComunes = time()\n",
    "viajes_comunes = spark.sql(\"\"\"\n",
    "    SELECT PUZone, DOZone, PUBorough, DOBorough\n",
    "      FROM datosCarrerasLimpios \n",
    "      WHERE PUBorough != 'Unknown' or DOBorough != 'Unknown'\n",
    "\"\"\")\n",
    "viajes_comun=viajes_comunes.rdd.map(\n",
    "    lambda x: (f\"De {x['PUZone']}({x['PUBorough']}) a {x['DOZone']}({x['DOBorough']})\", 1)\n",
    ").reduceByKey(\n",
    "    lambda a,b: a + b\n",
    ").sortBy(\n",
    "    lambda x: x[1], False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una aspecto a mencionar antes de continuar es que el viaje mas comun es el de Unknown a Unknown, cosa que no aporta demasiada informacion, por lo tanto estos viajes no se han tenido en cuenta de cara al analisis de los datos.\n",
    "\n",
    "Tras realizar la organización de los datos en tablas y realizar la reducción, se tomarán los 10 viajes más comunes y se\n",
    "graficarán como un gráfico de barras horizontal para poder visualizar las diferencias entre la cantidad de estos viajes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafica = viajes_comun.take(10)\n",
    "t1viajesComunes = time()    \n",
    "x, y = [],[]\n",
    "for i in grafica:    \n",
    "    x.append(i[0]),y.append(i[1])\n",
    "    \n",
    "plt.figure()\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_viajes_comunes=t1viajesComunes-t0viajesComunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propinas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a realizar un estudio sobre las propinas que dejan los pasajeros.\n",
    "Vamos a empezar contabilizando el numero de propinas que se dejan y clasificarlos por hora, para así poder ver en qué momento del día es más común que se dejen propinas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0Propinas=time()\n",
    "\n",
    "propinas_hora_query=spark.sql(\"\"\"\n",
    "  SELECT tip_amount, HOUR(tpep_dropoff_datetime) as hour \n",
    "      FROM datosCarrerasLimpios\n",
    "\"\"\")\n",
    "\n",
    "propinas_por_hora = propinas_hora_query.groupBy(\"hour\").count().collect()\n",
    "\n",
    "t1Propinas=time()\n",
    "\n",
    "propinas_x = [h for h, cnt in propinas_por_hora]\n",
    "propinas_y = [cnt for h, cnt in propinas_por_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(propinas_x, propinas_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Numero de propinas\", fontsize=25)\n",
    "ax.set_title(\"Numero de propinas por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, las horas más comunes son las cercanas a las 18h, es decir, a media tarde coincidiendo con la hora de salir de trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a realizar un estudio de la cantidad de dinero total (en miles de dólares) dejado en cada periodo para ver su similitud con la cantidad de propinas dejadas por hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2Propinas=time()\n",
    "propinas_totales_por_hora = propinas_hora_query.groupBy(\"hour\").sum('tip_amount').collect()\n",
    "\n",
    "t3Propinas=time()\n",
    "\n",
    "propinas_x = [h for h, cnt in propinas_totales_por_hora]\n",
    "propinas_y = [round(cnt/1000) for h, cnt in propinas_totales_por_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(propinas_x, propinas_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Propinas totales(k$)\", fontsize=25)\n",
    "ax.set_title(\"Propinas totales(k$) por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, la gráfica resultante es similar a la anterior, por lo que vemos que la cantidad de propinas dejadas tiene relación con la aportación monetaria que producen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, ahora vamos a hacer un estudio para ver en qué momento del día se dejan las propinas más cuantiosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4Propinas=time()\n",
    "propinas_maximas_por_hora = propinas_hora_query.groupBy(\"hour\").max('tip_amount').collect()\n",
    "\n",
    "t5Propinas=time()\n",
    "\n",
    "propinas_x = [h for h, cnt in propinas_maximas_por_hora]\n",
    "propinas_y = [cnt for h, cnt in propinas_maximas_por_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(propinas_x, propinas_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Propinas maximas($)\", fontsize=25)\n",
    "ax.set_title(\"Propinas maximas($) por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, hay propinas muy altas a las horas más comunes, pero otro conjunto de picos en la gráfica, nos indica que de madrugada se dejan también propinas muy cuantiosas, aunque no sean tan numerosas como a media tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempoPropinasTotal=(t5Propinas-t4Propinas)+(t3Propinas-t2Propinas)+(t1Propinas-t0Propinas)\n",
    "print(tiempoPropinasTotal,\"Segundos en realizarse el estudio de propinas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de tiempos, explicación y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tabla se recogen todos los tiempos calculados previamente en los estudios realizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{\"-\"*150}\n",
    "\\t\\tEstudio\\t\\t|\\tTiempo\\t\\t|\\tVelocidad procesamiento\\t\\t| Nº Transformaciones-Acciones\\t|\\tRDD/SQL \n",
    "{\"-\"*150}\n",
    "{\"-\"*150}\n",
    "\\tLimpieza\\t\\t|\\t{round(t_limpieza, 2)} s\\t\\t|\\t\\t{round(initial_data_count/t_limpieza, 2)}\\t\\t|\\t\\t3-4\\t\\t|\\tSQL\n",
    "{\"-\"*150}\n",
    "\\tVelocidad media taxis\\t|\\t{round(t_velocidad_media, 2)} s\\t\\t|\\t\\t{round(cantidad_velocidad_media/t_velocidad_media, 2)}\\t\\t|\\t\\t4-2\\t\\t|\\tSQL\n",
    "{\"~\"*150}\n",
    "\\tZonas sin cobertura\\t|\\t{round(t_cobertura, 2)} s\\t\\t|\\t\\t{round(datosLimpios_count/t_cobertura, 2)}\\t\\t|\\t\\t10-8\\t\\t|\\tHibrido\n",
    "{\"~\"*150}\n",
    "\\tPropinas\\t\\t|\\t{round(tiempoPropinasTotal, 2)} s\\t\\t|\\t\\t{round(datosLimpios_count/tiempoPropinasTotal, 2)}\\t\\t|\\t\\t5-4\\t\\t|\\tSQL\n",
    "{\"~\"*150}\n",
    "\\tViajes más comunes\\t|\\t{round(t_viajes_comunes, 2)} s\\t\\t|\\t\\t{round(c_viajes_comunes/t_viajes_comunes, 2)}\\t\\t|\\t\\t3-2\\t\\t|\\tHibrido\n",
    "{\"~\"*150}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, tenemos tiempos bastante dispares entre los distintos estudios. El más lento es el de Zonas sin cobertura y el más rápido el de Propinas. La diferencia de tiempos se debe principalmente a la complejidad tanto de los propios estudios como de las querys que en ellos se realizan. En este caso, en la más costosa se realizan varias acciones y transformaciones para sacar los registros guardados por distryto y por zona, después se busca cual es el día con más registros en el distrito con más registros para obtener la cantidad de mensajes guardados por hora. Ese estudio es de mayor complejidad que obtener las cantidades de proinas por hora, cantidad de éstas y el máximo en cada franja horaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
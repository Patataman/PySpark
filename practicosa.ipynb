{
 "cells": [
 "cells": [
  {
  {
   "cell_type": "markdown",
   "cell_type": "markdown",
   "metadata": {},
   "metadata": {},
   "source": [
   "source": [
    "# Imports"
    "# Imports"
   ]
   ]
  },
  },
  {
  {
   "cell_type": "code",
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "import sys, os\n",
    "import sys, os\n",
    "is_conda = os.path.exists(os.path.join(sys.prefix, 'conda-meta'))\n",
    "is_conda = os.path.exists(os.path.join(sys.prefix, 'conda-meta'))\n",
    "\n",
    "\n",
    "if not is_conda:\n",
    "if not is_conda:\n",
    "    import findspark \n",
    "    import findspark \n",
    "    findspark.init()\n",
    "    findspark.init()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# import pandas as pd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, datediff, unix_timestamp\n",
    "from pyspark.sql.functions import col, datediff, unix_timestamp\n",
    "import csv\n",
    "import csv\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Para una lectura más distendida de la memoria\n",
    "# Para una lectura más distendida de la memoria\n",
    "MODO_JAJAS = False"
    "MODO_JAJAS = True"
   ]
   ]
  },
  },
  {
  {
    "# Lectura de datos"
    "# Lectura de datos"
   ]
   ]
  },
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://giphy.com/embed/I1U9DTjCqOF3i\",width=\"240\", height=\"135\"))"
   ]
  },
  {
  {
   "cell_type": "code",
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
    "display(dfP.describe().T)"
    "display(dfP.describe().T)"
   ]
   ]
  },
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODO_JAJAS:\n",
    "    display(IFrame(\"https://giphy.com/embed/xsATxBQfeKHCg\", width=\"240\", height=\"180\"))"
   ]
  },
  {
  {
   "cell_type": "markdown",
   "cell_type": "markdown",
   "metadata": {},
   "metadata": {},
    "# Convertimos las fechas a timestamp, para que dejen de ser strings a secas\n",
    "# Convertimos las fechas a timestamp, para que dejen de ser strings a secas\n",
    "# y guardamos su diferencia para luego tener más fácil el filtrado y otros cálculos\n",
    "# y guardamos su diferencia para luego tener más fácil el filtrado y otros cálculos\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# ESTO ES ABSURDAMENTE LENTO, TIENE QUE HABER ALGUNA FORMA MÁS FÁCIL DE HACER ESTO\n",
    "# ----------------------------------------------------------------------------------\n",
    "df = df.withColumn(\n",
    "df = df.withColumn(\n",
    "    \"tpep_pickup_timestamp\", unix_timestamp(col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "    \"tpep_pickup_timestamp\", unix_timestamp(col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    ").withColumn(\n",
    ").withColumn(\n",
    "        improvement_surcharge >= 0 AND\n",
    "        improvement_surcharge >= 0 AND\n",
    "        time_diff > 0\n",
    "        time_diff > 0\n",
    "\"\"\")\n",
    "\"\"\")\n",
    "print(datosLimpios.count())\n",
    "# print(datosLimpios.count())\n",
    "datosLimpios.createOrReplaceTempView('datosCarrerasLimpios')\n",
    "datosLimpios.createOrReplaceTempView('datosCarrerasLimpios')\n",
    "datosLimpiosP = datosLimpios.toPandas()"
    "# datosLimpiosP = datosLimpios.toPandas()"
   ]
   ]
  },
  },
  {
  {
   },
   },
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "display(datosLimpiosP)\n",
    "# display(datosLimpiosP)\n",
    "display(datosLimpiosP.describe().T)"
    "# display(datosLimpiosP.describe().T)"
   ]
   ]
  },
  },
  {
  {
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "# Bajamos el csv con la información de las zonas para luego poder \"traducir\"\n",
    "if not os.path.exists(\"taxi+_zone_lookup.csv\"):\n",
    "if not os.path.exists(\"taxi+_zone_lookup.csv\"):\n",
    "    !wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
    "    !wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
   ]
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "# Diccionario para traducir el id de las localizaciones a cosas coherentes\n",
    "# Se añade al dataframe los datos de las zonas de subida y bajada. Para ello se va a realizar un join,\n",
    "# TODO CAMBIAR PARA HACERLO CON SPARK Y UN JOIN\n",
    "# De forma que vaya dentro del dataframe y es accesible también desde un rdd\n",
    "zone_lookup_csv = csv.DictReader(open(\"taxi+_zone_lookup.csv\"), delimiter=\",\")\n",
    "\n",
    "zone_lookup = {l['LocationID']: {\"distrito\": l['Borough'], \"zona\": l['Zone'], \"servicio\": l['service_zone']} for l in zone_lookup_csv}"
    "df_lookup = spark.read.csv('./taxi+_zone_lookup.csv', header=True, inferSchema=False)\n",
    "\n",
    "datosLimpios = datosLimpios.withColumn(\"LocationID\", col(\"PULocationID\")\n",
    ").join(\n",
    "    # Renombramos para poder luego incluir también las de bajada\n",
    "    df_lookup.withColumnRenamed(\"Borough\", \"PUBorough\"\n",
    "            ).withColumnRenamed(\"Zone\", \"PUZone\"\n",
    "            ).withColumnRenamed(\"service_zone\", \"PUservice_zone\"),\n",
    "    on=['LocationID']\n",
    ").withColumn(\"LocationID\", col(\"DOLocationID\")\n",
    ").join(\n",
    "    df_lookup.withColumnRenamed(\"Borough\", \"DOBorough\"\n",
    "            ).withColumnRenamed(\"Zone\", \"DOZone\"\n",
    "            ).withColumnRenamed(\"service_zone\", \"DOservice_zone\"),\n",
    "    on=['LocationID']\n",
    ")\n",
    "\n",
    "datosLimpios.createOrReplaceTempView('datosCarrerasLimpios')\n",
    "\n",
    "# datosLimpiosP = datosLimpios.toPandas()\n",
    "# display(datosLimpiosP)\n",
    "# display(datosLimpiosP.describe().T)"
   ]
   ]
  },
  },
  {
  {
   "cell_type": "markdown",
   "cell_type": "markdown",
   "metadata": {},
   "metadata": {},
   "source": [
   "source": [
    "### Zonas de poca cobertura 100% real link megaupload"
    "### Zonas de poca cobertura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la variable `store_and_fwd_flag`, creemos que es posible deducir qué zonas de la ciudad de Nueva York dan un mayor problema a la hora de estar conectados con el servidor de la compañía de taxis, es decir, tienen poca cobertura."
   ]
   ]
  },
  },
  {
  {
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "if MODO_JAJAS:\n",
    "if MODO_JAJAS:\n",
    "    display(HTML('<iframe src=\"https://giphy.com/embed/PmdOx0iRRtqkBFlEgI\" width=\"240\" height=\"240\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>'))"
    "    display(IFrame(\"https://giphy.com/embed/PmdOx0iRRtqkBFlEgI\", width=\"240\", height=\"240\"))"
   ]
   ]
  },
  },
  {
  {
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "sinCobertura = spark.sql(\"\"\"\n",
    "sinCobertura_rdd = spark.sql(\"\"\"\n",
    "    SELECT DOLocationID as locationID, store_and_fwd_flag\n",
    "    SELECT DOBorough, DOZone\n",
    "      FROM datosCarrerasLimpios\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE store_and_fwd_flag == 'Y'\n",
    "      WHERE store_and_fwd_flag == 'Y'\n",
    "\"\"\")\n",
    "\"\"\").rdd\n",
    "\n",
    "\n",
    "sC_rdd = sinCobertura.rdd\n",
    "# sC_rdd.flatMap(lambda x: x['locationID']).map(lambda x: (x,1))\n",
    "# sC_rdd.flatMap(lambda x: x['locationID']).map(lambda x: (x,1))\n",
    "zone_tuples = sC_rdd.map(\n",
    "zone_tuples = sinCobertura_rdd.map(\n",
    "    lambda x: (x['locationID'],1)\n",
    "    lambda x: (x['DOZone'],1)\n",
    ").reduceByKey(\n",
    "    lambda x,y: x+y\n",
    ").sortBy(\n",
    "    lambda x: x[1], False\n",
    ")\n",
    "borough_tuples = sinCobertura_rdd.map(\n",
    "    lambda x: (x['DOBorough'],1)\n",
    ").reduceByKey(\n",
    ").reduceByKey(\n",
    "    lambda x,y: x+y\n",
    "    lambda x,y: x+y\n",
    ").sortBy(\n",
    ").sortBy(\n",
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "distritos = defaultdict(int)\n",
    "def autolabel(rects):\n",
    "zonas = defaultdict(int)\n",
    "    # https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "for i, loc_id in enumerate(zone_tuples.collect()):\n",
    "    for rect in rects:\n",
    "    num = loc_id[1]\n",
    "        height = rect.get_height()\n",
    "    loc_id = str(loc_id[0])\n",
    "        ax.annotate('{}'.format(height),\n",
    "    zona = zone_lookup[loc_id]\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "    \n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "    distritos[zona['distrito']] += num\n",
    "                    textcoords=\"offset points\", fontsize=25,\n",
    "    zonas[zona['zona']] = num\n",
    "                    ha='center', va='bottom')"
    "\n",
   ]
    "distritos_x = [k for k in distritos.keys()]\n",
  },
    "distritos_y = [v for v in distritos.values()]\n",
  {
    "zonas_x = [k for k in zonas.keys()]\n",
   "cell_type": "code",
    "zonas_y = [v for v in zonas.values()]\n",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonas = zone_tuples.take(10)\n",
    "distritos = borough_tuples.take(10)\n",
    "\n",
    "\n",
    "distritos_x = [k for k, v in distritos]\n",
    "distritos_y = [v for k, v in distritos]\n",
    "zonas_x = [k for k, v in zonas]\n",
    "zonas_y = [v for k, v in zonas]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.bar(distritos_x, distritos_y)\n",
    "rects1 = ax.bar(distritos_x, distritos_y)\n",
    "ax.set_xlabel(\"Zona\")\n",
    "ax.set_xlabel(\"Distrito\", fontsize=25)\n",
    "ax.set_ylabel(\"Registros guardados\")\n",
    "ax.set_ylabel(\"Registros guardados\", fontsize=25)\n",
    "ax.set_title(\"Registros guardados por zona\")\n",
    "ax.set_title(\"Registros guardados por distritos\", fontsize=25)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.bar(zonas_x[:10], zonas_y[:10])\n",
    "rects2 = ax.bar(zonas_x, zonas_y)\n",
    "ax.set_xlabel(\"Zona\")\n",
    "ax.set_xlabel(\"Zona\", fontsize=25)\n",
    "ax.set_ylabel(\"Registros guardados\")\n",
    "ax.set_ylabel(\"Registros guardados\", fontsize=25)\n",
    "ax.set_title(\"Registros guardados por zona\")\n",
    "ax.set_title(\"Registros guardados por zonas\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viajes_manhattan = spark.sql(\"\"\"\n",
    "    SELECT VendorID\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE DOBorough == 'Manhattan'\n",
    "\"\"\").count()\n",
    "int(distritos_y[0])/int(viajes_manhattan)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar por el número de registros guardados en los taxis, Manhattan es el distrito que tiene más registros guardados con 2490, casi 5 veces más que su distrito posterior, Queens con 435. Por lo que se puede deducir que tiene zonas frecuentadas en las que no se tiene cobertura.\n",
    "\n",
    "Sin embargo, es un porcentaje pequeño sobre el total, ya que representan menos del 0.3% de todos los viajes existentes que finalizan en Manhattan, por lo que muy probablemente se deba a taxis concretos que tienen problemas de conexión.\n",
    "\n",
    "Observando que se trata de un porcentaje muy bajo, surge otra pregunta: ¿En qué momento se guardaron los registros? ¿Todos los registros guardados son en horas similares y por tanto puede deberse a una caída del servidor más que a un problema de cobertura?\n",
    "\n",
    "Para ello se van a observar los registros de Manhattan ya que son los más numerosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_manhattan = spark.sql(\"\"\"\n",
    "    SELECT tpep_dropoff_datetime, DAY(tpep_dropoff_datetime) as day, HOUR(tpep_dropoff_datetime) as hour\n",
    "      FROM datosCarrerasLimpios\n",
    "      WHERE store_and_fwd_flag == 'Y' and DOBorough == 'Manhattan'\n",
    "\"\"\")\n",
    "\n",
    "# Nos vamos a quedar con el día con mayor número de registros guardados\n",
    "day, count = stored_manhattan.groupBy(\"day\").count().sort(\"count\").tail(1)[0]\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "# Ahora que tenemos el día, podemos ver las horas en las que se agruparon los mensajes.\n",
    "# Si todos (o casi todos) los mensajes se agruparon en la misma hora, eso significa \n",
    "# (muy probablemente) que hubo una caída del servidor\n",
    "mensajes_hora = stored_manhattan.filter(col(\"day\")==day).groupBy(\"hour\").count().collect()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "mensajes_x = [h for h, cnt in mensajes_hora]\n",
    "mensajes_y = [cnt for h, cnt in mensajes_hora]\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "rects1 = ax.bar(mensajes_x, mensajes_y)\n",
    "ax.set_xlabel(\"Horas\", fontsize=25)\n",
    "ax.set_ylabel(\"Mensajes\", fontsize=25)\n",
    "ax.set_title(\"Mensajes guardados por hora\", fontsize=25)\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mensajes guardados se agrupan entre las 10 AM y las 21 PM, pero dado que se corresponde también con el periodo de actividad de los taxis, porque es cuando la gran mayoría de la población se desplaza por la ciudad, no parece que se deba a una caída del servidor.\n",
    "\n",
    "\n",
    "# print(f\"TOP {i+1} con {num}\\n    Distrito: {zona['distrito']}\\n    Zona: {zona['zona']}\\n    Servicio: {zona['servicio']}\")"
    "Por lo tanto nos reafirmamos en la primera suposición: **Muy probablemente se debe a taxis concretos que tienen problemas de conexión**, no ha una caída generalizada. Además si hubiese sido ese el caso, el número de registros debería haber sido muchísimo mayor"
   ]
   ]
  },
  },
  {
  {
@@ -430,7 +566,7 @@
   "metadata": {},
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
   "source": [
    "#spark.stop()"
    "spark.stop()"
   ]
   ]
  },
  },
  {
  {